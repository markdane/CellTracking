{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7c7V4yEqDc_",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Running cellpose 2.0 on exacloud with a GPU\n",
    "\n",
    "<font size = 4>Cellpose 2.0 now allows you to train your own models in the GUI!\n",
    "\n",
    "This notebook allows you to load this **custom model** and run the model on your images with a GPU. \n",
    "\n",
    "In this notebook, you can also **train** a custom model using your labels (`_seg.npy`) files, or other labels as `_masks.tif` files. If you already have a trained model, skip this part of the notebook.\n",
    "\n",
    "For more details on cellpose 2.0 check out the [paper](https://www.biorxiv.org/content/10.1101/2022.04.01.486764v1) or the [talk](https://www.youtube.com/watch?v=3ydtAhfq6H0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvyuR08OZfw4"
   },
   "source": [
    "# Setup\n",
    "\n",
    "We will first check the GPU is working, and mount google drive to get your models and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2cBEO1PLuO7"
   },
   "source": [
    "Check CUDA version and that GPU is working in cellpose and import other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tt8hgC7rniP8",
    "outputId": "677fa3d0-952f-4490-f5bb-4ef1ad0b0469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-06 11:35:05,018 [INFO] ** TORCH CUDA version installed and working. **\n",
      ">>> GPU activated? YES\n"
     ]
    }
   ],
   "source": [
    "#!nvcc --version\n",
    "#!nvidia-smi\n",
    "\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cellpose import core, utils, io, models, metrics\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "use_GPU = core.use_gpu()\n",
    "yn = ['NO', 'YES']\n",
    "print(f'>>> GPU activated? {yn[use_GPU]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "id": "uGUNrjdRfVDs"
   },
   "source": [
    "\n",
    "#@markdown ###Run this cell to connect your Google Drive to Colab\n",
    "\n",
    "#@markdown * Click on the URL. \n",
    "\n",
    "#@markdown * Sign in your Google Account. \n",
    "\n",
    "#@markdown * Copy the authorization code. \n",
    "\n",
    "#@markdown * Enter the authorization code. \n",
    "\n",
    "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n",
    "\n",
    "#mounts user's Google Drive to Google Colab.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1Ta76yatmjH"
   },
   "source": [
    "### Download sample images (optional)\n",
    "\n",
    "If you don't mount your google drive, and want to test cellpose 2.0, run the next code block to download the example data. This `human_in_the_loop` folder has a `train` folder with training images and manual segmentations (in this case created in the loop), and a `test` folder with test images and manual segmentations from scratch.\n",
    "\n",
    "These images are from the breast vectra class from the Tissuenet dataset. The full Tissuenet dataset is available [here](https://datasets.deepcell.org/), and for a description of the data see the [paper](https://www.nature.com/articles/s41587-021-01094-0).\n",
    "\n",
    "We will convert the `_seg.npy` files to `_masks.tif` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC7BIPW06z4t"
   },
   "outputs": [],
   "source": [
    "# !rm -rf human_in_the_loop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMG3YYFSdieb",
    "outputId": "d0ef685b-fbda-445c-e882-a53136794934"
   },
   "source": [
    "train_files = natsorted(glob('human_in_the_loop/train/*.tif'))\n",
    "train_seg = natsorted(glob('human_in_the_loop/train/*.npy'))\n",
    "\n",
    "test_files = natsorted(glob('human_in_the_loop/test/*.npy'))\n",
    "test_seg = natsorted(glob('human_in_the_loop/test/*.npy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j4m33Fuw5vm"
   },
   "source": [
    "what the training images look like + their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "ldNwr_zxMVha",
    "outputId": "83b17403-5ffc-4913-eb1e-7925b871f9ce"
   },
   "source": [
    "plt.figure(figsize=(12,4), dpi=300)\n",
    "for k,f in enumerate(train_files):\n",
    "    img = io.imread(f)\n",
    "    plt.subplot(2,len(train_files),k+1)\n",
    "    img = np.vstack((img, np.zeros_like(img)[:1]))\n",
    "    img = img.transpose(1,2,0)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2,len(train_files),len(train_files) + k+1)\n",
    "    seg = np.load(os.path.splitext(f)[0] + '_seg.npy', allow_pickle=True).item()\n",
    "    masks= seg['masks'].squeeze()\n",
    "    plt.imshow(masks)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfE75htF0l84"
   },
   "source": [
    "# Train model on manual annotations\n",
    "\n",
    "Skip this step if you already have a pretrained model.\n",
    "\n",
    "Fill out the form below with the paths to your data and the parameters to start training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLdKNWQ4jxy5"
   },
   "source": [
    "## Training parameters\n",
    "\n",
    "<font size = 4> **Paths for training, predictions and results**\n",
    "\n",
    "\n",
    "<font size = 4>**`train_dir:`, `test_dir`:** These are the paths to your folders train_dir (with images and masks of training images) and test_dir (with images and masks of test images). You can leave the test_dir blank, but it's recommended to have some test images to check the model's performance. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
    "\n",
    "<font size = 4>**`initial_model`:** Choose a model from the cellpose [model zoo](https://cellpose.readthedocs.io/en/latest/models.html#model-zoo) to start from.\n",
    "\n",
    "<font size = 4>**`model_name`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
    "\n",
    "<font size = 4>**Training parameters**\n",
    "\n",
    "<font size = 4>**`number_of_epochs`:** Input how many epochs (rounds) the network will be trained. At least 100 epochs are recommended, but sometimes 250 epochs are necessary, particularly from scratch. **Default value: 100**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQI4aUxCjz3n",
    "outputId": "804d0459-b120-4298-9b4c-87a9ca26401c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default advanced parameters enabled\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Ctn\"\n",
    "\n",
    "#@markdown ###Path to images and masks:\n",
    "data_path = \"/home/exacloud/gscratch/HeiserLab/images/cellpose_training\"\n",
    "train_dir = os.path.join(data_path, \"train\") #@param {type:\"string\"}\n",
    "test_dir = os.path.join(data_path, \"test\") #@param {type:\"string\"}\n",
    "#Define where the patch file will be saved\n",
    "base = \"/content\"\n",
    "\n",
    "# model name and path\n",
    "#@markdown ###Name of the pretrained model to start from and new model name:\n",
    "from cellpose import models\n",
    "initial_model = \"Ctn\" #@param ['cyto','nuclei','tissuenet','livecell','cyto2','CP','CPx','TN1','TN2','TN3','LC1','LC2','LC3','LC4','scratch']\n",
    "model_name = \"Ctn\" #@param {type:\"string\"}\n",
    "\n",
    "# other parameters for training.\n",
    "#@markdown ###Training Parameters:\n",
    "#@markdown Number of epochs:\n",
    "n_epochs =  100#@param {type:\"number\"}\n",
    "\n",
    "Channel_to_use_for_training = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "# @markdown ###If you have a secondary channel that can be used for training, for instance nuclei, choose it here:\n",
    "\n",
    "Second_training_channel= \"None\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "\n",
    "#@markdown ###Advanced Parameters\n",
    "\n",
    "Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n",
    "#@markdown ###If not, please input:\n",
    "learning_rate = 0.1 #@param {type:\"number\"}\n",
    "weight_decay = 0.0001 #@param {type:\"number\"}\n",
    "\n",
    "if (Use_Default_Advanced_Parameters): \n",
    "  print(\"Default advanced parameters enabled\")\n",
    "  learning_rate = 0.1 \n",
    "  weight_decay = 0.0001\n",
    "  \n",
    "#here we check that no model with the same name already exist, if so delete\n",
    "model_path = train_dir + 'models/'\n",
    "if os.path.exists(model_path+'/'+model_name):\n",
    "  print(\"!! WARNING: \"+model_name+\" already exists and will be deleted in the following cell !!\")\n",
    "  \n",
    "if len(test_dir) == 0:\n",
    "  test_dir = None\n",
    "\n",
    "# Here we match the channel to number\n",
    "if Channel_to_use_for_training == \"Grayscale\":\n",
    "  chan = 0\n",
    "elif Channel_to_use_for_training == \"Blue\":\n",
    "  chan = 3\n",
    "elif Channel_to_use_for_training == \"Green\":\n",
    "  chan = 2\n",
    "elif Channel_to_use_for_training == \"Red\":\n",
    "  chan = 1\n",
    "\n",
    "\n",
    "if Second_training_channel == \"Blue\":\n",
    "  chan2 = 3\n",
    "elif Second_training_channel == \"Green\":\n",
    "  chan2 = 2\n",
    "elif Second_training_channel == \"Red\":\n",
    "  chan2 = 1\n",
    "elif Second_training_channel == \"None\":\n",
    "  chan2 = 0\n",
    "\n",
    "if initial_model=='scratch':\n",
    "  initial_model = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8SDv9XztBgb"
   },
   "source": [
    "Here's what the command to train would be on the command line -- make sure if you run this locally to correct the paths for your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQsv-Iz7m_CF",
    "outputId": "bef9d0e1-ca51-450c-8c60-856c3f545bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m cellpose --use_gpu --verbose --train --dir /home/exacloud/gscratch/HeiserLab/images/cellpose_training/train --pretrained_model cyto2 --chan 0 --chan2 1 --n_epochs 100 --learning_rate 0.1 --weight_decay 0.0001 --test_dir /home/exacloud/gscratch/HeiserLab/images/cellpose_training/test --mask_filter _seg.npy\n"
     ]
    }
   ],
   "source": [
    "run_str = f'python -m cellpose --use_gpu --verbose --train --dir {train_dir} --pretrained_model {initial_model} --chan {chan} --chan2 {chan2} --n_epochs {n_epochs} --learning_rate {learning_rate} --weight_decay {weight_decay}'\n",
    "if test_dir is not None:\n",
    "    run_str += f' --test_dir {test_dir}'\n",
    "run_str += ' --mask_filter _seg.npy' # if you want to use _seg.npy files for training\n",
    "print(run_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JRxBPmatrK7"
   },
   "source": [
    "## Train new model\n",
    "\n",
    "Using settings from form above, train model in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcYskYudMajM",
    "outputId": "ed4dce67-190e-4d52-9a70-bcbc76be4b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-06 14:22:58,429 [INFO] WRITING LOG OUTPUT TO /home/users/dane/.cellpose/run.log\n",
      "2022-06-06 14:22:58,431 [INFO] >>>> loading model /home/exacloud/gscratch/HeiserLab/images/cellpose_training/train/models/Ctn\n",
      "2022-06-06 14:22:58,432 [INFO] ** TORCH CUDA version installed and working. **\n",
      "2022-06-06 14:22:58,434 [INFO] >>>> using GPU\n",
      "2022-06-06 14:22:58,571 [INFO] >>>> model diam_mean =  30.000 (ROIs rescaled to this size during training)\n",
      "2022-06-06 14:22:58,572 [INFO] >>>> model diam_labels =  15.371 (mean diameter of training ROIs)\n",
      "2022-06-06 14:22:58,578 [INFO] not all flows are present, running flow generation for all images\n",
      "2022-06-06 14:22:58,713 [INFO] 10 / 10 images in /home/exacloud/gscratch/HeiserLab/images/cellpose_training/train folder have labels\n",
      "2022-06-06 14:22:59,122 [INFO] computing flows for labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-06 14:23:00,169 [INFO] >>>> median diameter set to = 30\n",
      "2022-06-06 14:23:00,171 [INFO] >>>> mean of training label mask diameters (saved to model) 15.371\n",
      "2022-06-06 14:23:00,171 [INFO] >>>> training network with 2 channel input <<<<\n",
      "2022-06-06 14:23:00,172 [INFO] >>>> LR: 0.10000, batch_size: 8, weight_decay: 0.00010\n",
      "2022-06-06 14:23:00,173 [INFO] >>>> ntrain = 10\n",
      "2022-06-06 14:23:00,174 [INFO] >>>> nimg_per_epoch = 10\n",
      "2022-06-06 14:23:00,514 [INFO] Epoch 0, Time  0.3s, Loss 0.0791, LR 0.0000\n",
      "2022-06-06 14:23:00,789 [INFO] saving network parameters to /home/exacloud/gscratch/HeiserLab/images/cellpose_training/train/models/Ctn\n",
      "2022-06-06 14:23:01,990 [INFO] Epoch 5, Time  1.8s, Loss 0.0780, LR 0.0556\n",
      "2022-06-06 14:23:03,420 [INFO] Epoch 10, Time  3.2s, Loss 0.0626, LR 0.1000\n",
      "2022-06-06 14:23:06,233 [INFO] Epoch 20, Time  6.1s, Loss 0.0650, LR 0.1000\n",
      "2022-06-06 14:23:09,075 [INFO] Epoch 30, Time  8.9s, Loss 0.0544, LR 0.1000\n",
      "2022-06-06 14:23:11,899 [INFO] Epoch 40, Time 11.7s, Loss 0.0590, LR 0.1000\n",
      "2022-06-06 14:23:14,782 [INFO] Epoch 50, Time 14.6s, Loss 0.0690, LR 0.1000\n",
      "2022-06-06 14:23:17,608 [INFO] Epoch 60, Time 17.4s, Loss 0.0638, LR 0.1000\n",
      "2022-06-06 14:23:20,357 [INFO] Epoch 70, Time 20.2s, Loss 0.0712, LR 0.1000\n",
      "2022-06-06 14:23:23,234 [INFO] Epoch 80, Time 23.1s, Loss 0.0427, LR 0.1000\n",
      "2022-06-06 14:23:26,302 [INFO] Epoch 90, Time 26.1s, Loss 0.0742, LR 0.1000\n",
      "2022-06-06 14:23:28,872 [INFO] saving network parameters to /home/exacloud/gscratch/HeiserLab/images/cellpose_training/train/models/Ctn\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(data_path, \"train/models\",model_name) #@param {type:\"string\"}\n",
    "\n",
    "# start logger (to see training across epochs)\n",
    "logger = io.logger_setup()\n",
    "\n",
    "# declare model\n",
    "model = models.CellposeModel(gpu=True, \n",
    "                             pretrained_model=model_path,\n",
    "                            net_avg = True,\n",
    "                            diam_mean = 15.3)\n",
    "# set channels\n",
    "channels = [chan, chan2]\n",
    "\n",
    "# get files\n",
    "output = io.load_train_test_data(train_dir, mask_filter='_seg.npy')\n",
    "train_data, train_labels, _, test_data, test_labels, _ = output\n",
    "\n",
    "new_model_path = model.train(train_data, train_labels, \n",
    "                              #test_data=test_data,\n",
    "                              #test_labels=test_labels,\n",
    "                              channels=channels,\n",
    "                              save_path=train_dir, \n",
    "                              n_epochs=n_epochs,\n",
    "                              learning_rate=learning_rate, \n",
    "                              weight_decay=weight_decay, \n",
    "                              nimg_per_epoch=8,\n",
    "                              model_name=model_name)\n",
    "\n",
    "# diameter of labels in training images\n",
    "diam_labels = model.diam_labels.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdH0j8-L6FuB"
   },
   "source": [
    "## Evaluate on test data (optional)\n",
    "\n",
    "If you have test data, check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0AGsH5p6K6S",
    "outputId": "3f67063a-2197-4ec2-8619-0b52c98fd0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-06 14:25:15,038 [INFO] not all flows are present, running flow generation for all images\n",
      "2022-06-06 14:25:15,062 [INFO] 1 / 1 images in /home/exacloud/gscratch/HeiserLab/images/cellpose_training/test folder have labels\n",
      "\n",
      ">>> average precision at iou threshold 0.5 = 0.661\n"
     ]
    }
   ],
   "source": [
    "# get files (during training, test_data is transformed so we will load it again)\n",
    "output = io.load_train_test_data(test_dir, mask_filter='_seg.npy')\n",
    "test_data, test_labels = output[:2]\n",
    "\n",
    "# run model on test images\n",
    "masks = model.eval(test_data, \n",
    "                   channels=[chan, chan2],\n",
    "                   diameter=diam_labels)[0]\n",
    "\n",
    "# check performance using ground truth labels\n",
    "ap = metrics.average_precision(test_labels, masks)[0]\n",
    "print('')\n",
    "print(f'>>> average precision at iou threshold 0.5 = {ap[:,0].mean():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8tZ8uYR-IFW"
   },
   "source": [
    "plot masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Z2ac5gtr-HPq",
    "outputId": "65c96437-85e4-42cf-8d4b-414b6ba98c0a"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_244209/3051313018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_files = natsorted(glob(train_dir))\n",
    "plt.figure(figsize=(12,8), dpi=150)\n",
    "for k,im in enumerate(test_data):\n",
    "    img = im.copy()\n",
    "    plt.subplot(3,len(train_files), k+1)\n",
    "    img = np.vstack((img, np.zeros_like(img)[:1]))\n",
    "    img = img.transpose(1,2,0)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    if k==0:\n",
    "        plt.title('image')\n",
    "\n",
    "    plt.subplot(3,len(train_files), len(train_files) + k+1)\n",
    "    plt.imshow(masks[k])\n",
    "    plt.axis('off')\n",
    "    if k==0:\n",
    "        plt.title('predicted labels')\n",
    "\n",
    "    plt.subplot(3,len(train_files), 2*len(train_files) + k+1)\n",
    "    plt.imshow(test_labels[k])\n",
    "    plt.axis('off')\n",
    "    if k==0:\n",
    "        plt.title('true labels')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbVIZbNk5hgR"
   },
   "source": [
    "# Use custom model to segment images\n",
    "\n",
    "Take custom trained model from above\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "form",
    "id": "vDu4Ixjo588O"
   },
   "outputs": [],
   "source": [
    "# model name and path\n",
    "\n",
    "#@markdown ###Custom model path (full path):\n",
    "\n",
    "model_path = os.path.join(data_path, \"train/models\",model_name) #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ###Path to images:\n",
    "\n",
    "dir = os.path.join(data_path, \"image_cache\") #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "#@markdown ###Channel Parameters:\n",
    "\n",
    "Channel_to_use_for_segmentation = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "# @markdown If you have a secondary channel that can be used, for instance nuclei, choose it here:\n",
    "\n",
    "Second_segmentation_channel= \"None\" #@param [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
    "\n",
    "\n",
    "# Here we match the channel to number\n",
    "if Channel_to_use_for_segmentation == \"Grayscale\":\n",
    "  chan = 0\n",
    "elif Channel_to_use_for_segmentation == \"Blue\":\n",
    "  chan = 3\n",
    "elif Channel_to_use_for_segmentation == \"Green\":\n",
    "  chan = 2\n",
    "elif Channel_to_use_for_segmentation == \"Red\":\n",
    "  chan = 1\n",
    "\n",
    "\n",
    "if Second_segmentation_channel == \"Blue\":\n",
    "  chan2 = 3\n",
    "elif Second_segmentation_channel == \"Green\":\n",
    "  chan2 = 2\n",
    "elif Second_segmentation_channel == \"Red\":\n",
    "  chan2 = 1\n",
    "elif Second_segmentation_channel == \"None\":\n",
    "  chan2 = 0\n",
    "\n",
    "#@markdown ### Segmentation parameters:\n",
    "\n",
    "#@markdown diameter of cells (set to zero to use diameter from training set):\n",
    "diameter =  0#@param {type:\"number\"}\n",
    "#@markdown threshold on flow error to accept a mask (set higher to get more cells, e.g. in range from (0.1, 3.0), OR set to 0.0 to turn off so no cells discarded):\n",
    "flow_threshold = 0.4 #@param {type:\"slider\", min:0.0, max:3.0, step:0.1}\n",
    "#@markdown threshold on cellprob output to seed cell masks (set lower to include more pixels or higher to include fewer, e.g. in range from (-6, 6)):\n",
    "cellprob_threshold=0 #@param {type:\"slider\", min:-6, max:6, step:1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Axg2YQEpDx0e"
   },
   "source": [
    "if you're using the example test data we'll copy it to a new folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "InyKGtD3D2ZX",
    "outputId": "b1b4ecf3-41a0-4463-8944-afd8200cece0"
   },
   "source": [
    "src = 'human_in_the_loop/test'\n",
    "if dir[:len(src)] == src:\n",
    "    files = io.get_image_files(dir, '_masks')\n",
    "    dir = 'human_in_the_loop/eval/'\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    for f in files:\n",
    "        dst = dir + os.path.split(f)[1]\n",
    "        print(f'{f} > {dst}')\n",
    "        shutil.copyfile(f, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JJ1q0nTBAAR"
   },
   "source": [
    "Here's what the command to train would be on the command line -- make sure if you run this locally to correct the paths for your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8P5voZOVM-H9",
    "outputId": "a9c9f1fb-7cf9-4676-bc1a-8a261f888274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m cellpose --use_gpu --verbose --dir human_in_the_loop/eval/ --pretrained_model human_in_the_loop/train/models/CP_tissuenet --chan 2 --chan2 1 --diameter 0 --flow_threshold 0.4 --cellprob_threshold 0\n"
     ]
    }
   ],
   "source": [
    "run_str = f'python -m cellpose --use_gpu --verbose --dir {dir} --pretrained_model {model_path} --chan {chan} --chan2 {chan2} --diameter {diameter} --flow_threshold {flow_threshold} --cellprob_threshold {cellprob_threshold}'\n",
    "print(run_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN3rdsfMBc_8"
   },
   "source": [
    "## run custom model\n",
    "\n",
    "how to run the custom model in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCcbs722BYd0",
    "outputId": "b7de466b-4e7a-4585-b1d7-c282593b3fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00701_A1_1_004_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00701_A1_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00701_D2_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00701_D4_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00701_D6_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00801_A1_1_004_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00801_A1_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00801_D2_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00801_D4_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00801_D6_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00901_A1_1_004_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00901_A1_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00901_D2_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00901_D4_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC00901_D6_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01001_A1_1_004_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01001_A1_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01001_D2_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01001_D4_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01001_D6_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01301_A1_1_004_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01301_A1_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01301_D2_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01301_D4_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01301_D6_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01401_A1_1_004_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01401_A1_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01401_D2_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01401_D4_1_188_np_img.tif', '/home/exacloud/gscratch/HeiserLab/images/cellpose_training/image_cache/HC01401_D6_1_188_np_img.tif']\n",
      "2022-06-06 14:25:27,338 [INFO] >>>> loading model /home/exacloud/gscratch/HeiserLab/images/cellpose_training/train/models/Ctn\n",
      "2022-06-06 14:25:27,340 [INFO] ** TORCH CUDA version installed and working. **\n",
      "2022-06-06 14:25:27,341 [INFO] >>>> using GPU\n",
      "2022-06-06 14:25:27,465 [INFO] >>>> model diam_mean =  30.000 (ROIs rescaled to this size during training)\n",
      "2022-06-06 14:25:27,466 [INFO] >>>> model diam_labels =  15.371 (mean diameter of training ROIs)\n",
      "2022-06-06 14:25:27,469 [INFO] 0%|          | 0/30 [00:00<?, ?it/s]\n",
      "2022-06-06 14:25:29,131 [INFO] 3%|3         | 1/30 [00:01<00:48,  1.66s/it]\n",
      "2022-06-06 14:25:30,748 [INFO] 7%|6         | 2/30 [00:03<00:45,  1.64s/it]\n",
      "2022-06-06 14:25:32,203 [INFO] 10%|#         | 3/30 [00:04<00:41,  1.55s/it]\n",
      "2022-06-06 14:25:33,600 [INFO] 13%|#3        | 4/30 [00:06<00:38,  1.49s/it]\n",
      "2022-06-06 14:25:34,977 [INFO] 17%|#6        | 5/30 [00:07<00:36,  1.45s/it]\n",
      "2022-06-06 14:25:35,674 [INFO] No cell pixels found.\n",
      "2022-06-06 14:25:35,725 [INFO] 20%|##        | 6/30 [00:08<00:29,  1.21s/it]\n",
      "2022-06-06 14:25:36,677 [INFO] 23%|##3       | 7/30 [00:09<00:25,  1.13s/it]\n",
      "2022-06-06 14:25:37,667 [INFO] 27%|##6       | 8/30 [00:10<00:23,  1.08s/it]\n",
      "2022-06-06 14:25:38,833 [INFO] 30%|###       | 9/30 [00:11<00:23,  1.11s/it]\n",
      "2022-06-06 14:25:40,007 [INFO] 33%|###3      | 10/30 [00:12<00:22,  1.13s/it]\n",
      "2022-06-06 14:25:41,596 [INFO] 37%|###6      | 11/30 [00:14<00:24,  1.27s/it]\n",
      "2022-06-06 14:25:43,316 [INFO] 40%|####      | 12/30 [00:15<00:25,  1.41s/it]\n",
      "2022-06-06 14:25:44,848 [INFO] 43%|####3     | 13/30 [00:17<00:24,  1.44s/it]\n",
      "2022-06-06 14:25:46,385 [INFO] 47%|####6     | 14/30 [00:18<00:23,  1.47s/it]\n",
      "2022-06-06 14:25:47,735 [INFO] 50%|#####     | 15/30 [00:20<00:21,  1.44s/it]\n",
      "2022-06-06 14:25:49,101 [INFO] 53%|#####3    | 16/30 [00:21<00:19,  1.41s/it]\n",
      "2022-06-06 14:25:50,594 [INFO] 57%|#####6    | 17/30 [00:23<00:18,  1.44s/it]\n",
      "2022-06-06 14:25:52,034 [INFO] 60%|######    | 18/30 [00:24<00:17,  1.44s/it]\n",
      "2022-06-06 14:25:53,641 [INFO] 63%|######3   | 19/30 [00:26<00:16,  1.49s/it]\n",
      "2022-06-06 14:25:55,261 [INFO] 67%|######6   | 20/30 [00:27<00:15,  1.53s/it]\n",
      "2022-06-06 14:25:56,735 [INFO] 70%|#######   | 21/30 [00:29<00:13,  1.51s/it]\n",
      "2022-06-06 14:25:58,394 [INFO] 73%|#######3  | 22/30 [00:30<00:12,  1.56s/it]\n",
      "2022-06-06 14:26:00,024 [INFO] 77%|#######6  | 23/30 [00:32<00:11,  1.58s/it]\n",
      "2022-06-06 14:26:01,651 [INFO] 80%|########  | 24/30 [00:34<00:09,  1.59s/it]\n",
      "2022-06-06 14:26:03,297 [INFO] 83%|########3 | 25/30 [00:35<00:08,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# gets image files in dir (ignoring image files ending in _masks)\n",
    "files = io.get_image_files(dir, '_masks', imf = \"np_img\")\n",
    "print(files)\n",
    "images = [io.imread(f) for f in files]\n",
    "\n",
    "# declare model\n",
    "model = models.CellposeModel(gpu=True, \n",
    "                             pretrained_model=model_path)\n",
    "\n",
    "# use model diameter\n",
    "diameter = model.diam_labels \n",
    "\n",
    "# run model on test images\n",
    "masks, flows, styles = model.eval(images, \n",
    "                                  channels=[chan, chan2],\n",
    "                                  diameter=diameter,\n",
    "                                  flow_threshold=flow_threshold,\n",
    "                                  cellprob_threshold=cellprob_threshold\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj5AIZ825o7P"
   },
   "source": [
    "## save output to *_seg.npy\n",
    "\n",
    "you will see the files save in the Files tab and you can download them from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc7EWe_f5oEH"
   },
   "outputs": [],
   "source": [
    "from cellpose import io\n",
    "\n",
    "io.masks_flows_to_seg(images, \n",
    "                      masks, \n",
    "                      flows, \n",
    "                      diameter*np.ones(len(masks)), \n",
    "                      files, \n",
    "                      channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwoUuuarC9V5"
   },
   "source": [
    "## save output masks to tiffs/pngs or txt files for imageJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da-Rtx09DEZB"
   },
   "outputs": [],
   "source": [
    "io.save_masks(images, \n",
    "              masks, \n",
    "              flows, \n",
    "              files, \n",
    "              channels=channels,\n",
    "              png=True, # save masks as PNGs and save example image\n",
    "              tif=True, # save masks as TIFFs\n",
    "              save_txt=True, # save txt outlines for ImageJ\n",
    "              save_flows=False, # save flows as TIFFs\n",
    "              save_outlines=False, # save outlines as TIFFs \n",
    "              )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "PiP9MWN4F3Sx",
    "outputId": "e70088bf-dbb2-4b49-a5df-620c2b253d68"
   },
   "outputs": [],
   "source": [
    "f = files[0]\n",
    "plt.figure(figsize=(12,4), dpi=300)\n",
    "plt.imshow(io.imread(os.path.splitext(f)[0] + '_cp_masks.png'))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_cellpose_2.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
